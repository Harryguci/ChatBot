# CORS Configuration
FRONTEND_URL=http://localhost:3000

# Google Gemini API Configuration
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# Authentication Configuration
# Generate a secure secret key: python -c "import secrets; print(secrets.token_urlsafe(32))"
JWT_SECRET_KEY=change-this-to-a-secure-random-string-in-production
# Get Google OAuth Client ID from: https://console.cloud.google.com/apis/credentials
GOOGLE_CLIENT_ID=your-google-oauth-client-id.apps.googleusercontent.com
# Email address that should automatically receive admin role on first login
DEFAULT_ADMIN_EMAIL=admin@example.com

# Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_NAME=chatbot_db
DB_USER=postgres
DB_PASSWORD=postgres

# Database Connection Pool Configuration (Phase 1 Optimization)
# Increased from default 10 to 50 for better concurrency
DB_POOL_SIZE=50
# Increased from default 20 to 100 for peak load handling
DB_MAX_OVERFLOW=100
# Increased timeout from 30 to 60 seconds
DB_POOL_TIMEOUT=60
# Connection recycle time (1 hour)
DB_POOL_RECYCLE=3600

# Database Echo (set to true for SQL query logging)
DB_ECHO=false

# Redis Cache Configuration (Phase 1 Optimization)
# Enable/disable Redis caching (set to false if Redis is not available)
REDIS_ENABLED=true
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
# Optional: Redis password (leave empty if no password)
REDIS_PASSWORD=

# Semantic Cache Configuration
# Enable semantic similarity matching for cache hits (catches "What is Python?" and "What's Python?" as same query)
REDIS_SEMANTIC_CACHE=true
# Similarity threshold (0.0-1.0) - higher means stricter matching. Default: 0.95
# 0.95 = very similar queries, 0.85 = moderately similar, 0.75 = loosely similar
REDIS_SEMANTIC_THRESHOLD=0.95
# Embedding model for semantic caching (lightweight model recommended for performance)
# Options: paraphrase-multilingual-MiniLM-L12-v2 (default), all-MiniLM-L6-v2 (faster but English-only)
CACHE_EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# Performance Notes:
# - Total DB connections: DB_POOL_SIZE + DB_MAX_OVERFLOW = 150
# - Ensure PostgreSQL max_connections >= 200 (150 app + 50 admin)
# - Redis provides 40× faster cached query responses (2000ms → 50ms)
# - Rate limiting: 30 queries/min, 10 uploads/min per IP
# - LRU cache: 1000 most frequent query embeddings (~1.5 MB memory)
# - Expected improvement: 2-3× query throughput

# ===== RAG Enhancement Settings (v2.0) =====

# FAISS Vector Store Configuration
# Enable FAISS for in-memory vector similarity search (optional acceleration layer)
# Set to true for faster search (~100ms vs ~200ms), but requires more memory
USE_FAISS=false
# Directory for FAISS index persistence
FAISS_INDEX_PATH=./data/faiss

# Document Chunking Strategy
# Use LangChain RecursiveCharacterTextSplitter for semantic chunking
# Set to false to use legacy single-chunk strategy
USE_LANGCHAIN_CHUNKING=true
# Maximum characters per chunk (recommended: 1000-2000)
CHUNK_SIZE=1500
# Character overlap between chunks for context preservation (recommended: 10-20% of chunk_size)
CHUNK_OVERLAP=200

# OCR (Optical Character Recognition) Configuration
# Enable Tesseract OCR for scanned PDFs and low-quality documents
OCR_ENABLED=true
# Tesseract language codes (vie=Vietnamese, eng=English, use + to combine)
# Available: vie, eng, fra, deu, spa, etc. See: https://tesseract-ocr.github.io/tessdoc/Data-Files
OCR_LANGUAGES=vie+eng
# Minimum characters on a page before OCR is triggered (lower = more OCR, slower)
TEXT_THRESHOLD=150
# DPI for rendering PDF pages to images for OCR (higher = better quality, slower)
# Recommended: 300 (balance), 150 (fast), 600 (high quality)
PDF_DPI=300

# Query Enhancement - Multi-Query Retrieval
# Generate multiple query variations for better retrieval coverage
MULTI_QUERY_ENABLED=true
# Number of query variations to generate (1-10, recommended: 2-4)
NUM_QUERY_VARIATIONS=3

# Search Configuration
# Default number of results to return from similarity search
DEFAULT_TOP_K=5
# Minimum similarity score threshold (0.0-1.0, lower = more permissive)
# 0.1 = very permissive (includes weak matches)
# 0.5 = moderate (good balance)
# 0.7 = strict (only strong matches)
SIMILARITY_THRESHOLD=0.1
# Weight for recency boost (0.0-1.0, higher = favor newer documents)
# 0.0 = no recency boost, 0.15 = 15% weight to recency, 0.5 = 50% weight
RECENCY_WEIGHT=0.15

# PDF Processing
# Use enhanced PDF processor with PyMuPDF (fitz) for 3-5x faster extraction
USE_ENHANCED_PDF_PROCESSOR=true

# Model Configuration
# SentenceTransformer model for text embeddings
# Options: paraphrase-multilingual-MiniLM-L12-v2 (default), all-MiniLM-L6-v2 (faster, English-only)
TEXT_EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
# Gemini model for answer generation
# Options: gemini-2.0-flash-exp (default), gemini-1.5-pro, gemini-1.5-flash
LLM_MODEL=gemini-2.0-flash-exp

# RAG Enhancement Notes:
# - PyMuPDF: 3-5× faster PDF extraction vs PyPDF2
# - OCR: Handles scanned PDFs with >90% success rate
# - Semantic Chunking: Better granularity for long documents
# - Multi-Query: 10-15% improvement in retrieval accuracy
# - FAISS: Optional 2× faster search, requires ~2MB per 10k chunks
# - Migration: Run scripts/migrate_to_langchain_chunks.py for existing docs
